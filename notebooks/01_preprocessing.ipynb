{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "import albumentations as aug\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory to Kaggle's Alzheimer MRI Preprocessed Dataset.\n",
    "\n",
    "Link: https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset\n",
    "\n",
    "Alzheimer's Disease Dementia Classes: Non Demented, Very Mild Demented, Mild Demented, Moderate Demented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER PARENT PATH - Example: C:\\Users\\{USERNAME}\\ai-alzheimer-detection\n",
    "parent_path = r\"ENTER PARENT PATH\"\n",
    "\n",
    "kaggle_dir = r\"assets\\Kaggle\"\n",
    "kaggle_path = os.path.join(parent_path, kaggle_dir)\n",
    "\n",
    "kaggle_dataset_dir = r\"alzheimer_mri_preprocessed_dataset\"\n",
    "kaggle_raw_dir = r\"alzheimer_mri_preprocessed_dataset\\raw\"\n",
    "\n",
    "kaggle_dataset_path = os.path.join(kaggle_path, kaggle_dataset_dir)\n",
    "kaggle_raw_path = os.path.join(kaggle_path, kaggle_raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Non_Demented\", \"Very_Mild_Demented\", \"Mild_Demented\", \"Moderate_Demented\"]\n",
    "encoded_classes = {status: idx for idx, status in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent to path\n",
    "sys.path.append(parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading images and labels (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alzheimersdetection import Dataset\n",
    "\n",
    "X, y = Dataset.load_dataset(classes, kaggle_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.printShapes(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 80% training and 20% testing data. Ensure same class distribution using stratify=y (class/label).\n",
    "\n",
    "Further split the training data into 75% training and 25% validation respectively.\n",
    "\n",
    "Ratio: 60% Training : 20% Validation : 20% Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.80\n",
    "test_size = 0.20\n",
    "validation_size = 0.25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,  y, test_size=test_size, stratify=y)\n",
    "\n",
    "print(\"Before Validation - Training Data Shape:\", X_train.shape)\n",
    "print(\"Before Validation - Training Label Shape:\", y_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,  y_train, test_size=validation_size, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the overall sizes and verify the split occured correctly for each classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Size: \", X_train.shape)\n",
    "print(\"Test Size: \", X_test.shape)\n",
    "print(\"Validation Size: \", X_val.shape)\n",
    "\n",
    "print(\"\\nClasses encoded for reference: \", encoded_classes, \"\\n\")\n",
    "\n",
    "unique = np.unique(y, return_counts=True)\n",
    "print(\"Original: \", unique)\n",
    "\n",
    "unique = np.unique(y_train, return_counts=True)\n",
    "print(\"Training Split: \", unique)\n",
    "\n",
    "unique = np.unique(y_val, return_counts=True)\n",
    "print(\"Validation Split: \", unique)\n",
    "\n",
    "unique = np.unique(y_test, return_counts=True)\n",
    "print(\"Testing Split: \", unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the testing and validation data onto a compressed Numpy Archive (*.npz)\n",
    "\n",
    "Image is saved for viewing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = X_test.min()\n",
    "print(max_val)\n",
    "\n",
    "test_dir = fr\"{kaggle_dataset_path}\\Test_Data\"\n",
    "test_npz = \"test_data.npz\"\n",
    "Dataset.save_images_npz(X_test, y_test, classes, test_dir, test_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir = fr\"{kaggle_dataset_path}\\Validation_Data\"\n",
    "validation_npz = \"val_data.npz\"\n",
    "Dataset.save_images_npz(X_val, y_val, classes, validation_dir, validation_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stats.statistics as Statistics\n",
    "\n",
    "title = \"AD Classification Distribution with Training Dataset\"\n",
    "\n",
    "unique = np.unique(y_train, return_counts=True)\n",
    "size = unique[1].tolist()\n",
    "\n",
    "sample_dist = (classes, size)\n",
    "\n",
    "print(encoded_classes)\n",
    "print(unique)\n",
    "Statistics.pieChartClassificationPlot(sample_dist, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further balance the dataset, we need to employ more techniques. One of which is data augmentation.\n",
    "Method to balance the data augmentation process is to define class-specific augmentation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = aug.Compose(\n",
    "    [\n",
    "        aug.Resize(height=128, width=128),\n",
    "        aug.HorizontalFlip(p=0.5),\n",
    "        aug.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=0, border_mode=cv2.BORDER_CONSTANT, value=0, p=1),\n",
    "        aug.RandomBrightnessContrast(brightness_limit=0.0, contrast_limit=0.4, p=1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_aug, y_aug = [], []\n",
    "rates = [1, 1, 2, 3]\n",
    "\n",
    "# Function to augment\n",
    "def augment(image, transform):\n",
    "    augmented_image = transform(image=np.array(image))[\"image\"]  # Extract augmented image from Albumentations output\n",
    "    return augmented_image\n",
    "\n",
    "for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
    "    for j in range(rates[label]):\n",
    "        augment_image = augment(image, data_transforms)\n",
    "        X_aug.append(augment_image)\n",
    "        y_aug.append(label)\n",
    "\n",
    "X_aug = np.array(X_aug)\n",
    "y_aug = np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, X_aug), axis=0)\n",
    "y_train = np.concatenate((y_train, y_aug), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distribution after Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"AD Classification Distribution after Data Augmentation\"\n",
    "\n",
    "unique = np.unique(y_train, return_counts=True)\n",
    "size = unique[1].tolist()\n",
    "\n",
    "sample_dist = (classes, size)\n",
    "\n",
    "print(encoded_classes)\n",
    "print(unique)\n",
    "Statistics.pieChartClassificationPlot(sample_dist, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape Before:\", X_train.shape)\n",
    "\n",
    "# Preprocess data (normalize pixel values)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[1] * X_train.shape[2]))  # Reshape for normalization\n",
    "\n",
    "print(\"Shape After: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is still imbalanced and to fix this, we need to increase the minority class's representation (oversampling). This allows us to have a more balanced dataset.\n",
    "\n",
    "We will be using Adaptive Synthetic Sampling (ADASYN) to oversample the minority classes.\n",
    "\n",
    "Optimal Results: ~25% distribution across all AD classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AD Classification Distribution before Oversampling\")\n",
    "class_counts = Counter(y_train)\n",
    "print(class_counts)\n",
    "\n",
    "# Visualize class imbalance before oversampling\n",
    "title = \"Class Distribution before Oversampling\"\n",
    "x_label = \"CDR Rating\"\n",
    "y_label = \"Number of Images\"\n",
    "Statistics.barClassificationPlot(sample=class_counts, title=title, x_label=x_label, y_label=y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 # This is the k-neighbors which will be used for ADASYN\n",
    "\n",
    "adasyn = ADASYN(n_neighbors=k)\n",
    "\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Original dataset shape counter: %s\" % Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_resampled.shape)\n",
    "\n",
    "# Reshape the 2d np array back to a 3d np array\n",
    "size = X_resampled.shape[0]\n",
    "X_resampled = X_resampled.reshape(size, 128, 128)\n",
    "\n",
    "# Invert normalization\n",
    "X_resampled = (X_resampled * 255).astype(np.uint8)  # Scale back to 0-255 and convert to uint8 for PyTorch\n",
    "\n",
    "print(X_resampled.shape)\n",
    "Dataset.showImage(X_resampled, len(X_resampled)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distribution after oversampling\n",
    "print(\"AD Classification Distribution after Oversampling\")\n",
    "class_counts_balanced = Counter(y_resampled)\n",
    "print(class_counts_balanced)\n",
    "\n",
    "Dataset.printShapes(X_resampled, y_resampled)\n",
    "\n",
    "# Visualize class distribution after oversampling\n",
    "title = \"Class Distribution after Oversampling\"\n",
    "Statistics.barClassificationPlot(sample=class_counts_balanced, title=title, x_label=x_label, y_label=y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training data onto a compressed Numpy Archive (*.npz)\n",
    "\n",
    "Image is saved for viewing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = fr\"{kaggle_dataset_path}\\Train_Data\"\n",
    "train_npz = \"augmented_adasyn_train_data.npz\"\n",
    "Dataset.save_images_npz(X_resampled, y_resampled, classes, train_dir, train_npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "<p style=\"text-align: center;\"> Made with ❤️ </p>\n",
    "<p style=\"text-align: center;\"> Darwin Xue </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
