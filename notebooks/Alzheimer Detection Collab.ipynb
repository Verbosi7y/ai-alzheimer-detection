{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alzheimer Detection using Google Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/Verbosi7y/ai-alzheimer-detection.git\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-image\n",
    "%pip install scikit-learn\n",
    "%pip install imbalanced-learn\n",
    "%pip install albumentations\n",
    "%pip install opencv-python\n",
    "%pip install pillow\n",
    "\n",
    "# Uncomment if you are running Google Colab on a CUDA GPU (NVIDIA)\n",
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = r\"/content/ai-alzheimer-detection\"\n",
    "\n",
    "kaggle_dir = r\"assets/Kaggle\"\n",
    "kaggle_path = os.path.join(parent_path, kaggle_dir)\n",
    "\n",
    "kaggle_dataset_dir = r\"alzheimer_mri_preprocessed_dataset\"\n",
    "kaggle_raw_dir = r\"alzheimer_mri_preprocessed_dataset/raw\"\n",
    "\n",
    "kaggle_dataset_path = os.path.join(kaggle_path, kaggle_dataset_dir)\n",
    "kaggle_raw_path = os.path.join(kaggle_path, kaggle_raw_dir)\n",
    "\n",
    "model_dir = r\"models\"\n",
    "model_path = os.path.join(parent_path, model_dir)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "model_dir = r\"models/best_ad_model.pth\"\n",
    "model_path = os.path.join(parent_path, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent to path\n",
    "sys.path.append(parent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alzheimersdetection import Dataset\n",
    "\n",
    "X, y = Dataset.step1_load_data(path=kaggle_raw_path) # np.array, np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 80% training and 20% testing data. Ensure same class distribution using stratify=y (class/label).\n",
    "\n",
    "Further split the training data into 75% training and 25% validation respectively.\n",
    "\n",
    "Ratio: 60% Training : 20% Validation : 20% Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "validation_size = 0.25\n",
    "\n",
    "split_dataset = Dataset.step2_split_data(X, y, test_size=test_size, validation_size=validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization for the Distribution of the Training Dataset\n",
    "\n",
    "Results should be heavily imbalanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stats.statistics as Statistics\n",
    "\n",
    "title_before_aug = \"AD Classification Distribution of Training Dataset\"\n",
    "\n",
    "sample_dist = Dataset.distribution(split_dataset[\"train\"][\"y\"])\n",
    "\n",
    "Statistics.pieChartClassificationPlot(sample_dist, title_before_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Balance and Oversample the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further balance the dataset, we need to employ more techniques. One of which is data augmentation.\n",
    "Method to balance the data augmentation process is to define class-specific augmentation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Rates:\n",
    "    - Non_Demented: 1\n",
    "    - Very_Mild_Demented: 1\n",
    "    - Mild_Demented: 2\n",
    "    - Moderate_Demented: 5\n",
    "'''\n",
    "rates = [1, 1, 2, 5]\n",
    "\n",
    "split_dataset[\"train\"] = Dataset.step3a_augmentation(split_dataset[\"train\"], rates=rates)\n",
    "\n",
    "Dataset.display_split(split_dataset=split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing out results of the class distribution after data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_after_aug = \"AD Classification Distribution after Data Augmentation\"\n",
    "\n",
    "aug_dist = Dataset.distribution(split_dataset[\"train\"][\"y\"])\n",
    "\n",
    "Statistics.pieChartClassificationPlot(aug_dist, title_after_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. ADASYN Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is still imbalanced and to fix this, we need to increase the minority class's representation (oversampling). This allows us to have a more balanced dataset.\n",
    "\n",
    "We will be using Adaptive Synthetic Sampling (ADASYN) to oversample the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class imbalance before ADASYN\n",
    "title_before_ADASYN = \"Class Distribution before ADASYN\"\n",
    "\n",
    "Dataset.display_split(split_dataset=split_dataset);\n",
    "\n",
    "Statistics.ad_plot_bar(sample=split_dataset[\"train\"], title=title_before_ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Adapative Synthetic Sampling (ADASYN)\n",
    "\n",
    "Optimal Results: ~25% distribution across all AD classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 # This is the k-neighbors which will be used for ADASYN\n",
    "\n",
    "split_dataset[\"train\"] = Dataset.step3b_ADASYN(sample=split_dataset[\"train\"], k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing our results after applying ADASYN as a Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class imbalance after ADASYN\n",
    "title_after_ADASYN = \"Class Distribution after ADASYN\"\n",
    "\n",
    "Dataset.display_split(split_dataset=split_dataset);\n",
    "\n",
    "Statistics.ad_plot_bar(sample=split_dataset[\"train\"], title=title_after_ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save as dataset .npz and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset.step4_save_npz(split_dataset, path=kaggle_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "        \"epoches\"       : 25, # implement early stopping\n",
    "        \"learning_rate\" : 0.001,\n",
    "        \"batch_size\"    : 8,\n",
    "        \"early_stop\"    : 5\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Load Dataset as Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alzheimersdetection.AlzheimerDataset import AlzheimerDataset\n",
    "\n",
    "train_dataset = AlzheimerDataset(samples=split_dataset[\"train\"])\n",
    "val_dataset = AlzheimerDataset(samples=split_dataset[\"test\"])\n",
    "test_dataset = AlzheimerDataset(samples=split_dataset[\"validation\"])\n",
    "\n",
    "loaders =  {\n",
    "           \"train\"  : DataLoader(train_dataset, batch_size=param[\"batch_size\"], shuffle=True),\n",
    "           \"test\"   : DataLoader(val_dataset, batch_size=param[\"batch_size\"], shuffle=False),\n",
    "           \"val\"    : DataLoader(test_dataset, batch_size=param[\"batch_size\"], shuffle=False)\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Device Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device Setup:\n",
    "\n",
    "If you have an Nvidia GPU, you need to install CUDA\n",
    "\n",
    "Otherwise, CPU will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alzheimersdetection import AlzheimerModel\n",
    "\n",
    "device = AlzheimerModel.set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Creating CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our model using PyTorch's Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alzheimersdetection.AlzheimerModel import AlzheimerCNN\n",
    "\n",
    "model = AlzheimerCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterion: Cross Entropy Loss\n",
    "\n",
    "Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlzheimerModel.step9_train_model(model, param, loaders, device, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Verify using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alzheimersdetection import AlzheimerMetrics\n",
    "\n",
    "AlzheimerMetrics.run_metrics(model, loaders[\"test\"], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "<p style=\"text-align: center;\"> Made with ❤️ </p>\n",
    "<p style=\"text-align: center;\"> Darwin Xue </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
